{
  "cells": [
    {
      "metadata": {
        "_uuid": "225708f447eee93041881f9d6c3a3e890cb16718"
      },
      "cell_type": "markdown",
      "source": "Final LGBM Model for CS542 & Kaggle_Two_Sigma"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3cf24e5e4916804424a07bb0a94261ce2599f752"
      },
      "cell_type": "code",
      "source": "# Reduce data load for code test\ntoy = False",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c20fa6deeac9d374c98774abd90bdc76b023ee63"
      },
      "cell_type": "code",
      "source": "(market_train_df1, news_train_df1) = env.get_training_data()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e349943cc34c29c4e9605d293e9986d5e1e9bc3"
      },
      "cell_type": "code",
      "source": "market_train_df1.shape, news_train_df1.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c94396ce11b8410eefac366138641a61fa2f186"
      },
      "cell_type": "code",
      "source": "import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\n# n_features_h = 30\n# n_features_sub = 15\n# n_features_aud = 10\n\n# hv_h = HashingVectorizer(n_features=n_features_h, strip_accents='ascii')\n# hv_sub = HashingVectorizer(n_features=n_features_sub, strip_accents='ascii')\n# hv_aud = HashingVectorizer(n_features=n_features_aud, strip_accents='ascii')\n\n# column_names_h = [\"h_feat{0}\".format(i) for i in range(1, n_features_h+1)]\n# column_names_sub = [\"sub_feat{0}\".format(i) for i in range(1, n_features_sub+1)]\n# column_names_aud = [\"aud_feat{0}\".format(i) for i in range(1, n_features_aud+1)]\n\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "e1c3dc282b158fa96d79ff407517f694dbc48899"
      },
      "cell_type": "code",
      "source": "\nmarket_train_df1['close_to_open'] =  np.abs(market_train_df1['close'] / market_train_df1['open'])\nmarket_train_df1['assetName_mean_open'] = market_train_df1.groupby('assetName')['open'].transform('mean')\nmarket_train_df1['assetName_mean_close'] = market_train_df1.groupby('assetName')['close'].transform('mean')\n\n# If close is unrealistically far from open, replace with avg close\nfor i, row in market_train_df1.loc[market_train_df1['close_to_open'] >= 2].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df1.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df1.iloc[i,4] = row['assetName_mean_close']\n\n# If open is unrealistically far from close, replace with avg open\nfor i, row in market_train_df1.loc[market_train_df1['close_to_open'] <= 0.5].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df1.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df1.iloc[i,4] = row['assetName_mean_close']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b65d07db7d61281e90b3d07a6a1402686315c3ae"
      },
      "cell_type": "code",
      "source": "#Box Plots\n# columns = ['bodySize','wordCount','relevance','sentimentNegative','sentimentNeutral','sentimentPositive','noveltyCount24H','volumeCounts24H']\n# news_train_df1.boxplot(column=None, by=None, ax=None, fontsize=None, rot=0, grid=True, figsize=None, layout=None, return_type=None)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fba8c359ca9f08fe0f0ab824c325ed9ab77c9e63"
      },
      "cell_type": "code",
      "source": "# Reduce the number of samples for memory reasons\nif toy:\n    market_train_df = market_train_df1.sample(100_000)\n    news_train_df = news_train_df1.sample(300_000)\nelse:\n    market_train_df = market_train_df1.sample(2_000_000)\n    news_train_df = news_train_df1.sample(2_000_000)\n\ndel market_train_df1\ndel news_train_df1\n    \nmarket_train_df.shape, news_train_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e01d8497b2aaea8fe8a0021517b37355c405cb65",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Converting to float to conserve memory\nfloat_cols = {c: 'float32' for c in market_train_df.columns if c not in ['assetCode', 'time','assetName']}\nmarket_train_df.astype(float_cols)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f8174e0fe351b1c2c667fb463406975e941aba9c"
      },
      "cell_type": "code",
      "source": "random_val = lambda x: x.sample(1).reset_index(drop=True)\n\nnews_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f5b943eba18b52eb8447009469981478fba49028"
      },
      "cell_type": "code",
      "source": "\n# UNCOMMENT ALL THIS BLOCK IF NO NLP\n\n# for i in range(1, n_features_h+1):\n#     news_cols_agg[\"h_feat{0}\".format(i)] = ['mean']\n\n# for i in range(1, n_features_sub+1):\n#     news_cols_agg[\"sub_feat{0}\".format(i)] = ['mean']\n\n# for i in range(1, n_features_aud+1):\n#     news_cols_agg[\"aud_feat{0}\".format(i)] = ['min', 'max', 'mean']\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f710e7c7c0eda1484c3446fa45b00d284e90d16d"
      },
      "cell_type": "code",
      "source": "# UNCOMMENT ALL THIS BLOCK IF NO NLP\n\n# def text_feat_extract (news_data):\n#     \"\"\"function to process the news text data. Jon Hale\"\"\"\n#     news_data.headline = news_data.headline.str.lower()\n#     news_data.subjects = news_data.subjects.str.lower()\n#     news_data.audiences = news_data.audiences.str.lower()\n\n#     news_data['headline'] = list(hv_h.transform(news_data.headline))\n#     h_features = np.array(list(map(lambda x: x.toarray(), news_data.headline)))\n#     h_features = h_features.reshape((h_features.shape[0],n_features_h))\n#     news_data[column_names_h] = pd.DataFrame(h_features, index=news_data.headline.index)\n#     del h_features\n#     news_data.drop(['headline'], axis=1, inplace=True)\n\n#     news_data['subjects'] = list(hv_sub.transform(news_data.subjects))\n#     sub_features = np.array(list(map(lambda x: x.toarray(), news_data.subjects)))\n#     sub_features = sub_features.reshape((sub_features.shape[0],n_features_sub))\n#     news_data[column_names_sub] = pd.DataFrame(sub_features, index=news_data.subjects.index)\n#     del sub_features\n#     news_data.drop(['subjects'], axis=1, inplace=True)\n\n#     news_data['audiences'] = list(hv_aud.transform(news_data.audiences))\n#     aud_features = np.array(list(map(lambda x: x.toarray(), news_data.audiences)))\n#     aud_features = aud_features.reshape((aud_features.shape[0],n_features_aud))\n#     news_data[column_names_aud] = pd.DataFrame(aud_features, index=news_data.audiences.index)\n#     del aud_features\n#     news_data.drop(['audiences'], axis=1, inplace=True)\n    \n#     news_data_new = news_data.copy()\n#     del news_data\n\n#     return news_data_new\n\n# news_train_df = text_feat_extract(news_train_df)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c65c69b28db9b55f64a3088cb611908212e66aef",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# Even if not using full NLP, extract headline word count and then drop column\nnews_train_df['head_word_count'] = news_train_df['headline'].str.split('[\\W_]+').str.len()\n# # news_train_df['head_word_count'].apply(lambda x: pd.value_counts(.split(\"[\\W_]+\"))).sum(axis = 0)\nnews_train_df.drop(['headline'], axis = 1, inplace = True)\nnews_train_df.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c5a87dde9b7931382da32d0b362116fb8fbc94c"
      },
      "cell_type": "code",
      "source": "def join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n    \n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time','assetCode']).agg(news_cols_agg)\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time','assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    \n    return market_train_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "969511254abbe99bfbb38c4e85422596635cbd30"
      },
      "cell_type": "code",
      "source": "def get_xy(market_train_df, news_train_df, le=None):\n    x, le = get_x(market_train_df, news_train_df)\n    y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1.2)\n    return x, y, le\n\n\ndef label_encode(series, min_count):\n    vc = series.value_counts()\n    le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n    return le\n\n\ndef get_x(market_train_df, news_train_df, le=None):\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n    # Round time of market_train_df to 0h of curret day\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n\n    # Join market and news\n    x = join_market_news(market_train_df, news_train_df)\n    \n    # If not label-encoder... encode assetCode\n    if le is None:\n        le_assetCode = label_encode(x['assetCode'], min_count=10)\n        le_assetName = label_encode(x['assetName'], min_count=5)\n    else:\n        # 'unpack' label encoders\n        le_assetCode, le_assetName = le\n        \n#     x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n#     x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n    \n    try:\n        x.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n    except:\n        pass\n    try:\n        x.drop(columns=['universe'], inplace=True)\n    except:\n        pass\n    x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n    x.drop(columns='time', inplace=True)\n#    x.fillna(-1000,inplace=True)\n\n    # Fix some mixed-type columns\n    for bogus_col in ['marketCommentary_min', 'marketCommentary_max']:\n        x[bogus_col] = x[bogus_col].astype(float)\n    \n    return x, (le_assetCode, le_assetName)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31cd42303a383d2a6537c6f7b99ec2d298f26ff7"
      },
      "cell_type": "code",
      "source": "%%time\n\n# This will take some time...\nX, y, le = get_xy(market_train_df, news_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "969d2f159c075d73660850d143d7fd1a19fa0915"
      },
      "cell_type": "code",
      "source": "X.shape, y.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "068cd43c59e905593126201737f8a82d1024cdfb"
      },
      "cell_type": "code",
      "source": "# Save universe data for latter use\nuniverse = market_train_df['universe']\ntime = market_train_df['time']\n\n# Free memory\ndel market_train_df, news_train_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5142e49fb4156dddd15740beb4d38ecd23398247"
      },
      "cell_type": "code",
      "source": "X_ = X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "76afa018beace8447edbe3ccd904aa8b8b772e4c"
      },
      "cell_type": "code",
      "source": "# Keep only text columns\nX = X_#.iloc[:, X.columns.get_loc('urgency_min'):X.columns.get_loc('dayofweek')]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ffccbc2cf61684731578abd3e1108bff691c23a0"
      },
      "cell_type": "code",
      "source": "X.head(100)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "676b28bbf482d680e642c6c7450bf5c918399467"
      },
      "cell_type": "code",
      "source": "n_train = int(X.shape[0] * 0.75)\n\nX_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\nX_valid, y_valid = X.iloc[n_train:], y.iloc[n_train:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8a172258a46c7db96e85964d6d56c24f0ec56f0c"
      },
      "cell_type": "code",
      "source": "# For valid data, keep only those with universe > 0. This will help calculate the metric\nu_valid = (universe.iloc[n_train:] > 0)\nt_valid = time.iloc[n_train:]\n\nX_valid = X_valid[u_valid]\ny_valid = y_valid[u_valid]\nt_valid = t_valid[u_valid]\ndel u_valid",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31cd42303a383d2a6537c6f7b99ec2d298f26ff7"
      },
      "cell_type": "code",
      "source": "# Creat lgb datasets\ntrain_cols = X.columns.tolist()\ncategorical_cols = [] # ['assetCode', 'assetName', 'dayofweek', 'month']\n\n# Note: y data is expected to be a pandas Series, as we will use its group_by function in `sigma_score`\ndtrain = lgb.Dataset(X_train.values, y_train, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\ndvalid = lgb.Dataset(X_valid.values, y_valid, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "30794623829aa27afe5725c53bcc1941a7a64dc8"
      },
      "cell_type": "code",
      "source": "# We will 'inject' an extra parameter in order to have access to df_valid['time'] inside sigma_score without globals\ndvalid.params = {\n    'extra_time': t_valid.factorize()[0]\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c152c25d8447512d1ee1befa0c0757a8b7cc54f4",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "lgb_params = dict(\n    objective = 'regression_l1',\n    learning_rate = 0.1,\n    num_leaves = 300,\n    max_depth = -1,\n#     min_data_in_leaf = 1000,\n#     min_sum_hessian_in_leaf = 10,\n    bagging_fraction = 0.75,\n    max_bin = 300,\n    bagging_freq = 2,\n    feature_fraction = 0.5,\n    lambda_l1 = 0.0,\n    lambda_l2 = 1.0,\n    metric = 'None', # This will ignore the loss objetive and use sigma_score instead,\n    seed = 42 # Change for better luck! :)\n)\n\ndef sigma_score(preds, valid_data):\n    df_time = valid_data.params['extra_time']\n    labels = valid_data.get_label()\n    \n#    assert len(labels) == len(df_time)\n\n    x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n    \n    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n    # is a pd.Series and call `group_by`\n    x_t_sum = x_t.groupby(df_time).sum()\n    score = x_t_sum.mean() / x_t_sum.std()\n\n    return 'sigma_score', score, True\n\nevals_result = {}\nm = lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), verbose_eval=25,\n              early_stopping_rounds=200, feval=sigma_score, evals_result=evals_result)\n\n\ndf_result = pd.DataFrame(evals_result['valid'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e969dd0e1d6e590fb82788673a0fc82fb096ae2a"
      },
      "cell_type": "code",
      "source": "ax = df_result.plot(figsize=(12, 8))\nax.scatter(df_result['sigma_score'].idxmax(), df_result['sigma_score'].max(), marker='+', color='red')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e969dd0e1d6e590fb82788673a0fc82fb096ae2a"
      },
      "cell_type": "code",
      "source": "num_boost_round, valid_score = df_result['sigma_score'].idxmax()+1, df_result['sigma_score'].max()\nprint(lgb_params)\nprint(f'Best score was {valid_score:.5f} on round {num_boost_round}')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b6bf2745074d13c564596e99ad7170b72d626df",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(1, 2, figsize=(14, 14))\nlgb.plot_importance(m, ax=ax[0])\nlgb.plot_importance(m, ax=ax[1], importance_type='gain')\nfig.tight_layout()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "79f77f74583568d57c5caec118ffa8e00687b1e3"
      },
      "cell_type": "markdown",
      "source": "Training full model..."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1a4cf693020f85edc4d3a762656e09276b807aef"
      },
      "cell_type": "code",
      "source": "# Train full model\ndtrain_full = lgb.Dataset(X, y, feature_name=train_cols, categorical_feature=categorical_cols)\n\nmodel = lgb.train(lgb_params, dtrain, num_boost_round=num_boost_round)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f8925ddb4c42201fdba38efac258ca38773849d"
      },
      "cell_type": "code",
      "source": "def make_predictions(predictions_template_df, market_obs_df, news_obs_df, le):\n    x, _ = get_x(market_obs_df, news_obs_df, le)\n    predictions_template_df.confidenceValue = np.clip(model.predict(x), -1, 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "724c38149860c8e9058474ac9045c2301e8a20da",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "days = env.get_prediction_days()\n\nprint(news_obs_df.keys)\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    make_predictions(predictions_template_df, market_obs_df, news_obs_df, le)\n    env.predict(predictions_template_df)\nprint('Done!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"
      },
      "cell_type": "code",
      "source": "env.write_submission_file()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}