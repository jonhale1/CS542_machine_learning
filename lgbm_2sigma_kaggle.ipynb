{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "225708f447eee93041881f9d6c3a3e890cb16718"
   },
   "source": [
    "# LGBM Model for Two Sigma Kaggle Competition 12/05/18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3cf24e5e4916804424a07bb0a94261ce2599f752"
   },
   "outputs": [],
   "source": [
    "# Will reduce data load for code test\n",
    "toy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3400a017f53cd879e48a394c1b794e3c7ffa5ba"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "localtime = time.asctime( time.localtime(time.time()) )\n",
    "print (\"Local current time :\", localtime) #Its in GMT? Interesting I wonder if this is kaggle or 2sigma hardware "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from kaggle.competitions import twosigmanews\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = twosigmanews.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c20fa6deeac9d374c98774abd90bdc76b023ee63"
   },
   "outputs": [],
   "source": [
    "(market_train_df, news_train_df) = env.get_training_data()\n",
    "market_train_df.shape, news_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f5bb3cf72499e64d3d8834cf68fe39fd27ee2f1b"
   },
   "outputs": [],
   "source": [
    "market_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c26076c49e839780dd8502439d5b29f6cd5679a0"
   },
   "outputs": [],
   "source": [
    "market_train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c9897a0c97f7552c22304e46376b37c8a90a190"
   },
   "outputs": [],
   "source": [
    "news_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "765d8f05649bcba2a67ca6532e621646e444581a"
   },
   "outputs": [],
   "source": [
    "news_train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fba8c359ca9f08fe0f0ab824c325ed9ab77c9e63"
   },
   "outputs": [],
   "source": [
    "# Reduce the number of samples for memory reasons\n",
    "if toy:\n",
    "    market_train_df = market_train_df.tail(100_000)\n",
    "    news_train_df = news_train_df.tail(300_000)\n",
    "else:\n",
    "    market_train_df = market_train_df.tail(3_000_000)\n",
    "    news_train_df = news_train_df.tail(6_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b092c146da2a745bf03866d22467b09a83b2730a"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8174e0fe351b1c2c667fb463406975e941aba9c"
   },
   "outputs": [],
   "source": [
    "news_cols_agg = {\n",
    "    'urgency': ['min', 'count'],\n",
    "    'takeSequence': ['max'],\n",
    "    'bodySize': ['min', 'max', 'mean', 'std'],\n",
    "    'wordCount': ['min', 'max', 'mean', 'std'],\n",
    "    'sentenceCount': ['min', 'max', 'mean', 'std'],\n",
    "    'companyCount': ['min', 'max', 'mean', 'std'],\n",
    "    'marketCommentary': ['min', 'max', 'mean', 'std'],\n",
    "    'relevance': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f492c1e3a7d2fc304d6dad012c6866e61ffcc70a"
   },
   "outputs": [],
   "source": [
    "news_train_df['assetCodes'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c5a87dde9b7931382da32d0b362116fb8fbc94c"
   },
   "outputs": [],
   "source": [
    "def join_market_news(market_train_df, news_train_df):\n",
    "    # Fix asset codes (str -> list)\n",
    "    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n",
    "    \n",
    "    # Expand assetCodes\n",
    "    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n",
    "    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n",
    "\n",
    "    assert len(assetCodes_index) == len(assetCodes_expanded)\n",
    "    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n",
    "\n",
    "    # Create expandaded news (will repeat every assetCodes' row)\n",
    "    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n",
    "    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n",
    "\n",
    "    # Free memory\n",
    "    del df_assetCodes, news_train_df\n",
    "\n",
    "    # Aggregate numerical news features\n",
    "    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n",
    "    \n",
    "    # Free memory \n",
    "    del news_train_df_expanded\n",
    "    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n",
    "\n",
    "    # Flat columns\n",
    "    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n",
    "    \n",
    "    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n",
    "\n",
    "    # Free memory\n",
    "    del news_train_df_aggregated\n",
    "    \n",
    "    return market_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "969511254abbe99bfbb38c4e85422596635cbd30"
   },
   "outputs": [],
   "source": [
    "def get_xy(market_train_df, news_train_df, le=None):\n",
    "    x, le = get_x(market_train_df, news_train_df)\n",
    "    y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1)\n",
    "    return x, y, le\n",
    "\n",
    "\n",
    "def label_encode(series, min_count):\n",
    "    vc = series.value_counts()\n",
    "    le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n",
    "    return le\n",
    "\n",
    "\n",
    "def get_x(market_train_df, news_train_df, le=None):\n",
    "    # Split date into before and after 22h (the time used in train data)\n",
    "    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n",
    "    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n",
    "    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n",
    "\n",
    "    # Round time of market_train_df to 0h of curret day\n",
    "    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n",
    "\n",
    "    # Join market and news\n",
    "    x = join_market_news(market_train_df, news_train_df)\n",
    "    \n",
    "    # If not label-encoder... encode assetCode\n",
    "    if le is None:\n",
    "        le_assetCode = label_encode(x['assetCode'], min_count=10)\n",
    "        le_assetName = label_encode(x['assetName'], min_count=5)\n",
    "    else:\n",
    "        # 'unpack' label encoders\n",
    "        le_assetCode, le_assetName = le\n",
    "        \n",
    "    x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n",
    "    x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n",
    "    \n",
    "    try:\n",
    "        x.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        x.drop(columns=['universe'], inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n",
    "    x.drop(columns='time', inplace=True)\n",
    "\n",
    "    # Fix some mixed-type columns\n",
    "    for bogus_col in ['marketCommentary_min', 'marketCommentary_max']:\n",
    "        x[bogus_col] = x[bogus_col].astype(float)\n",
    "    \n",
    "    return x, (le_assetCode, le_assetName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31cd42303a383d2a6537c6f7b99ec2d298f26ff7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This will take some time...\n",
    "X, y, le = get_xy(market_train_df, news_train_df)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "068cd43c59e905593126201737f8a82d1024cdfb"
   },
   "outputs": [],
   "source": [
    "# Save universe data for later use\n",
    "universe = market_train_df['universe']\n",
    "time = market_train_df['time']\n",
    "\n",
    "# Free memory\n",
    "del market_train_df, news_train_df\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ffccbc2cf61684731578abd3e1108bff691c23a0"
   },
   "outputs": [],
   "source": [
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "676b28bbf482d680e642c6c7450bf5c918399467"
   },
   "outputs": [],
   "source": [
    "n_train = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\n",
    "X_valid, y_valid = X.iloc[n_train:], y.iloc[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a172258a46c7db96e85964d6d56c24f0ec56f0c"
   },
   "outputs": [],
   "source": [
    "# For valid data, keep only those in universe the i.e > 0\n",
    "u_valid = (universe.iloc[n_train:] > 0)\n",
    "t_valid = time.iloc[n_train:]\n",
    "\n",
    "X_valid = X_valid[u_valid]\n",
    "y_valid = y_valid[u_valid]\n",
    "t_valid = t_valid[u_valid]\n",
    "\n",
    "#clears memory\n",
    "del u_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31cd42303a383d2a6537c6f7b99ec2d298f26ff7"
   },
   "outputs": [],
   "source": [
    "# Creat lgb datasets\n",
    "train_cols = X.columns.tolist()\n",
    "categorical_cols = [] \n",
    "\n",
    "# Note: y data is expected to be a pandas Series, as we will use its group_by function in `sigma_score`\n",
    "dtrain = lgb.Dataset(X_train.values, y_train, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\n",
    "dvalid = lgb.Dataset(X_valid.values, y_valid, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\n",
    "\n",
    "# We will 'inject' an extra parameter in order to have access to df_valid['time'] inside sigma_score without globals\n",
    "dvalid.params = {\n",
    "    'extra_time': t_valid.factorize()[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c152c25d8447512d1ee1befa0c0757a8b7cc54f4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create GBM parameters for LGBM model\n",
    "lgb_params = dict(\n",
    "    objective = 'regression_l1',\n",
    "    learning_rate = 0.1,\n",
    "    num_leaves = 127,\n",
    "    max_depth = -1,\n",
    "#     min_data_in_leaf = 1000,\n",
    "#     min_sum_hessian_in_leaf = 10,\n",
    "    bagging_fraction = 0.75,\n",
    "    bagging_freq = 2,\n",
    "    feature_fraction = 0.5,\n",
    "    lambda_l1 = 0.0,\n",
    "    lambda_l2 = 1.0,\n",
    "    metric = 'None', # This will ignore the loss objetive and use sigma_score instead,\n",
    "    seed = 42 # Change for better luck! :)\n",
    ")\n",
    "\n",
    "def sigma_score(preds, valid_data):\n",
    "    df_time = valid_data.params['extra_time']\n",
    "    labels = valid_data.get_label()\n",
    "    \n",
    "    x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n",
    "    \n",
    "    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n",
    "    # is a pd.Series and call `group_by`\n",
    "    \n",
    "    x_t_sum = x_t.groupby(df_time).sum()\n",
    "    score = x_t_sum.mean() / x_t_sum.std()\n",
    "\n",
    "    return 'sigma_score', score, True\n",
    "\n",
    "evals_result = {}\n",
    "m = lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), verbose_eval=25,\n",
    "              early_stopping_rounds=100, feval=sigma_score, evals_result=evals_result)\n",
    "\n",
    "\n",
    "df_result = pd.DataFrame(evals_result['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e969dd0e1d6e590fb82788673a0fc82fb096ae2a"
   },
   "outputs": [],
   "source": [
    "ax = df_result.plot(figsize=(12, 8))\n",
    "ax.scatter(df_result['sigma_score'].idxmax(), df_result['sigma_score'].max(), marker='+', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e969dd0e1d6e590fb82788673a0fc82fb096ae2a"
   },
   "outputs": [],
   "source": [
    "num_boost_round, valid_score = df_result['sigma_score'].idxmax()+1, df_result['sigma_score'].max()\n",
    "print(lgb_params)\n",
    "print(f'Best score was {valid_score:.5f} on round {num_boost_round}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b6bf2745074d13c564596e99ad7170b72d626df",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 14))\n",
    "lgb.plot_importance(m, ax=ax[0])\n",
    "lgb.plot_importance(m, ax=ax[1], importance_type='gain')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a4cf693020f85edc4d3a762656e09276b807aef"
   },
   "outputs": [],
   "source": [
    "# Use optimal nu_round to train full model\n",
    "dtrain_full = lgb.Dataset(X, y, feature_name=train_cols, categorical_feature=categorical_cols)\n",
    "\n",
    "model = lgb.train(lgb_params, dtrain, num_boost_round=num_boost_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f8925ddb4c42201fdba38efac258ca38773849d"
   },
   "outputs": [],
   "source": [
    "def make_predictions(predictions_template_df, market_obs_df, news_obs_df, le):\n",
    "    x = get_x(market_obs_df, news_obs_df, le)[0]\n",
    "    predictions_template_df.confidenceValue = np.clip(model.predict(x), -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "724c38149860c8e9058474ac9045c2301e8a20da"
   },
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()\n",
    "\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    make_predictions(predictions_template_df, market_obs_df, news_obs_df, le)\n",
    "    env.predict(predictions_template_df)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"
   },
   "outputs": [],
   "source": [
    "# Write submission file to Kaggle environment\n",
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7453ae03d316bcbcb23c1da48655e5709f288421"
   },
   "outputs": [],
   "source": [
    "# Complete submission file\n",
    "import os\n",
    "print([filename for filename in os.listdir('.') if '.csv' in filename])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
